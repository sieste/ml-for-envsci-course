\documentclass{beamer}

\input{preamble.tex}


\title{Introduction to Machine Learning for Environmental Science}


\begin{document}

\maketitle

\begin{frame}
\textbf{This 2-day workshop introduces core concepts of machine learning
including common algorithms, standard workflows and key software
libraries. The focus will be on supervised machine learning for
regression and classification using a range of methods such as random
forests and deep convolutional neural networks. The material will be
taught through a mix of theory and practical examples across different
environmental sciences such as meteorology and ecology.}
\end{frame}

\section{Introduction}

\subsection{Supervised ML}


\begin{frame}{Some examples}
\textbf{Air quality prediction}: Predict high-resolution spatial pattern of next day PM10 concentration based on learned relationships between pollution, weather and traffic.
\begin{center}
\includegraphics[width=.5\textwidth]{fig/air-quality-map.png}
\end{center}
\end{frame}

\begin{frame}{Examples of ML applications}
\textbf{Gap filling}: Fill in missing data due to clouds in satellite images based on learned spatial patterns. 
\begin{center}
\includegraphics[width=.5\textwidth]{fig/cloud-inpainting.png}
\end{center}
\begin{flushright}
{\tiny https://doi.org/10.3390/rs12233865}
\end{flushright}
\end{frame}


\begin{frame}{Examples of ML applications}
\textbf{Monitoring wildlife}: Learning patterns in seismic signals to detect and monitor animal species.
\begin{center}
\includegraphics[width=.7\textwidth]{fig/animal-sounds.png}
\end{center}
\begin{flushright}
{\tiny https://doi.org/10.1111/2041-210X.70021}
\end{flushright}
\end{frame}


\begin{frame}{Examples of ML applications}
\textbf{Weather prediction}: Learning spatial, temporal and intervariable relationships between weather quantities to make skilful global weather predictions.
\begin{center}
\includegraphics[width=.6\textwidth]{fig/graphcast.png}
\end{center}
\begin{flushright}
{\tiny https://doi.org/10.48550/arXiv.2212.12794}
\end{flushright}
\end{frame}

\begin{frame}{Examples of ML applications}
\textbf{Image segmentation}: Detecting and annotating trees on urban maps based on learned image patterns.
\begin{center}
\includegraphics[width=.4\textwidth]{fig/tree-segmentation.png}
\end{center}
\begin{flushright}
{\tiny https://doi.org/10.3390/ijgi11040226}
\end{flushright}
\end{frame}

\begin{frame}{Examples of ML applications}
What do these examples have in common?

\begin{itemize}
\item High-dimensional data (space, time, multivariable)
\item Need to learn complex interactions between variables
\item Data-rich but often limited mechanistic understanding
\item Clear framing as an input-output relationship
\item Focus on skilful prediction of observables
\end{itemize}

\end{frame}


\begin{frame}{Supervised machine learning: General setup}
\begin{itemize}
\item We have an observable \textbf{target outcome} we wish to predict. 
\item The prediction can be based on a set of \textbf{input features}. 
\item For example
\begin{itemize}
\item predict future temperature based on current temperature, pressure, wind 
\item predict occurrence of wildfire based on measures of heat, humidity, wind 
\item predict crop yield based on agricultural practice, soil nutrients, pests
\end{itemize}
\item The goal of supervised ML is to \textbf{build a model} that predicts outcomes based on available features.
\item A "good" ML model is one that produces accurate predictions.
\end{itemize}
\end{frame}


\begin{frame}{Regression and classification}
\begin{itemize}
\item The type of target variable determines the type of ML problem.
\item \textbf{Regression:} Predicting quantitative outcomes. 
\begin{itemize}
\item For example: temperature in degrees, crop yield in tonnes, number of sharks, rain amount in mm
\end{itemize}
\item \textbf{Classification:} Predicting categorical outcomes. 
\begin{itemize}
\item For example: wildfire yes/no, temperature freezing/nonfreezing, precipitation type rain/snow/hail, air quality low/medium/high
\end{itemize}
\item Regression vs classification determines various details of the ML model and analysis.
\end{itemize}
\end{frame}


\begin{frame}{WIP: Regression and classification}

TODO: example outputs from a regression model and a classification model used later in the course to illustrate the distinction

\end{frame}


\subsection{Model training}

\begin{frame}{Function approximation theory}
\begin{itemize}
\item A ML model is a \textbf{prediction rule} that translates one or more features \(x_1, x_2, \dots\) into an estimate \(\hat{y}\) of the target \(y\).
\item The prediction rule is calculated by a \textbf{mathematical function} \[\hat{y} = f(x_1, x_2, \dots)\]
\item The function \(f(\dots)\) usually includes \textbf{model parameters} \(\theta_1, \theta_2, \dots\) that control how inputs are translated to outputs.
\item Defining the feature vector \(\fat{x} = (x_1, x_2, \dots)\) and parameter vector \(\fat{\theta}=(\theta_1, \theta_2, \dots)\) a trainable ML model is often compactly written as \[\hat{y} = f(\fat{x}; \fat\theta)\]
\end{itemize}
\end{frame}


\begin{frame}{Training data}
\begin{itemize}
\item Supervised ML works by "learning from examples".
\item \textbf{Training data} \(S\) is a set of input-output examples \[S = \left\{(\fat{x}_i, y_i)\right\}_{i=1}^n\] where
\begin{itemize}
\item \(\fat{x}_i = (x_{i,1}, x_{i,2}, \dots, x_{i,k})\) is the \textbf{feature vector} (containing \(k\) features) in the \(i\)-th example
\item \(y_i\) is the \textbf{target outcome} observed in the \(i\)-th example
\item \(n\) is the \textbf{sample size}
\end{itemize}
\item The data set \(S\) is used to train our ML model \(f(\fat{x};\fat\theta)\) that can predict the outcome \(y^*\) for arbitrary feature vector \(\fat{x}^*\).
\end{itemize}
\end{frame}

\begin{frame}{Loss function}
\begin{itemize}
\item A loss function \(L(\hat{y},y)\) is a function that quantifies \textbf{how different} the prediction \(\hat{y}\) is to the target outcome \(y\).
\item Loss functions are typically \textbf{negatively oriented}, where lower values indicate better predictions.
\item The loss function must be appropriate for the ML task at hand (regression vs classification) and can usually be set when intialising the ML model.
\item Otherwise the choice of loss function is \textbf{often ambiguous} and difficult to fully justify a particular choice.
\end{itemize}
\end{frame}

\begin{frame}{Loss function}
\begin{itemize}
\item \textbf{Regression} (where \(\hat{y}\) and \(y\) are real numbers):
\begin{itemize}
\item Squared-error loss \(L(\hat{y},y) = (\hat{y} - y)^2\)
\item Absolute-error loss \(L(\hat{y},y) = |\hat{y} - y|\)
\end{itemize}
\item \textbf{Binary classification} (where \(y\) is 0 or 1 and \(\hat{y}\) is the probability for outcome "1"):
\begin{itemize}
\item Cross-entropy loss \(L(\hat{y},y) = -y\log\hat{y} - (1-y)\log(1-\hat{y})\)
\item Quadratic loss \(L(\hat{y},y) = (\hat{y} - y)^2\)
\end{itemize}
\item \textbf{Multivariate classification} (where \(y = (0,\dots,1,\dots,0)\) is a binary vector indicating which of \(k\) classes occurs and \(\hat{y}\) is a vector of \(k\) class probabilities)
\begin{itemize}
\item Cross-entropy loss \(L(\hat{y},y) = - \sum_{i=1}^k y_k \log \hat{y}_k\)
\item Focal loss \(L(\hat{y},y) = - \sum_{i=1}^k y_k (1-\hat{y})^\gamma \log \hat{y}_k\)
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Empirical loss minimisation}
\begin{itemize}
\item We can now \textbf{formalise} training of ML model \(f(\fat{x};\fat\theta)\) as
\begin{itemize}
\item trying to find model parameters \(\fat\theta\) such that
\item the model outputs \(\hat{y}_1, ..., \hat{y}_n\) calculated from features \(\fat{x}_1, \dots, \fat{x}_n\) 
\item achieve minimal average loss when compared to the targets \(y_1, \dots, y_n\) in the training data set
\item using an appropriate loss function \(L(\hat{y}, y)\).
\end{itemize}
\item \textbf{The Empirical Loss Minimisation paradigm:}
\[\hat{\fat\theta} = \underset{\fat\theta}{\mathrm{argmin}}\frac1n\sum_{i=1}^n L\left[f(\fat{x}_i; \fat\theta), y_i\right]\]
\item The trained model with parameters \(\hat{\fat\theta}\) can then be used to make predictions \(\hat{y}^*\) based on new feature vectors \(\fat{x}^*\)
\[\hat{y}^* = f(\fat{x}^*; \hat{\fat\theta})\]
\end{itemize}
\end{frame}

\begin{frame}{In summary ...}
To train a ML model by empirical loss minimisation we need:
\begin{itemize}
\item a training data set \(S\) containing \(n\) examples of inputs and corresponding outcomes
\item a model function \(f(\fat{x};\fat\theta)\) with trainable parameters \(\fat\theta\) that translates inputs \(\fat{x}\) into an output \(\hat{y}\)
\item a loss function \(L(\hat{y},y)\) that quantifies how close model output \(\hat{y}\) is to the target outcome \(y\)
\item a search algorithm to find parameters that minimise the loss 
\end{itemize}
The combination of training data set, search algorithm, model function \(f(\fat{x};\fat\theta)\) and loss function \(L(\hat{y},y)\) determine the ML approach and \textbf{these details should always be reported when documenting ML results.}
\end{frame}

\subsection{Examples, model evaluation}


\begin{frame}{Code example: Linear regression}
\begin{itemize}
\item \textbf{Features} \(x_1, \dots, x_n\): temperatures at different times \(t_1, \dots, t_n\)
\item \textbf{Targets} \(y_1, \dots, y_n\): temperatures a short time \(\Delta\) later \(t_1 + \Delta, \dots, t_n + \Delta\)
\item \textbf{Model}: Linear function with intercept \(\theta_1\) and slope \(\theta_2\), such that \[\hat{y} = f(x; \fat\theta) = \theta_1 + \theta_2 \cdot x\]
\item \textbf{Loss}: Squared-error loss \[L(\hat{y}, y) = (\hat{y} - y)^2\]
\item \textbf{Search}: Exaustive search over a grid of parameter values (don't do this in practice!)
\end{itemize}
\end{frame}

\begin{frame}{Code example: Linear regression}
\begin{center}
\includegraphics[width=.28\textwidth]{fig/linreg-xy.png}
\includegraphics[width=.3\textwidth]{fig/linreg-xyplot1.png}
\end{center}

\begin{center}
\includegraphics[width=.37\textwidth]{fig/linreg-thetaheatmap.png}
\includegraphics[width=.35\textwidth]{fig/linreg-xyplot2.png}
\includegraphics[width=.25\textwidth]{fig/linreg-xyyhat.png}
\end{center}
\end{frame}

\begin{frame}[fragile]{Machine Learning in python with \texttt{scikit-learn}}
\begin{center}
\texttt{https://scikit-learn.org}
\includegraphics[width=\textwidth]{fig/sklearn.png}
\end{center}
\end{frame}


\begin{frame}[fragile]{Linear Regression in Scikit-Learn}
\begin{lstlisting}[language=python]
import numpy as np
from sklearn.linear_model import LinearRegression

# load file
t7110 = np.loadtxt("t7110.dat", comments="#")

# extract features (july temperatures) and targets (august temperatures)
x = t7110[:, 7]
y = t7110[:, 8]

# sklearn expects k features to be stored in a 2d array with k columns
x = x.reshape(-1, 1)   

# initialise a linear regression model
model = LinearRegression()

# fit the model to data
model.fit(x, y)

# extract parameter estimates from fitted model
theta_hat = np.array([model.intercept_, model.coef_[0] ])

# calculate predictions in training data
y_hat = model.predict(x)

# calculate predictions on new data
x_new = np.linspace(15, 20, 100).reshape(-1,1)
y_new = model.predict(x_new)
\end{lstlisting}
\end{frame}



\begin{frame}{How good is my model?}
\begin{itemize}
\item The model outputs \(\hat{y}_1, \dots, \hat{y}_n\) be compared to the targets \(y_1, \dots, y_n\) using a suitable error metric.
\item For example root mean squared error (RMSE) \[RMSE = \sqrt{\frac1n \sum_{i=1}^n (\hat{y}_i - y_i)^2}\]
\item Our model has \(RMSE \approx 0.9\) -- Is that "good"?
\item Getting \(RMSE > 0\) means our model is not perfect, but how bad is it?
\item To judge whether the model is useful we should compare it to a suitable benchmark. 
\end{itemize}
\end{frame}


\begin{frame}{Benchmarking}
\begin{itemize}
\item A benchmark model (or reference model) for performance evaluation is usually one or several of
\begin{itemize}
\item An alternative (competing) model for the same target.
\item Climatology (constant mean): \[\hat{y}^{(clim)}_i = \frac1n \sum_{i=1}^n y_i\]
\item Persistence (last available observation of the target); here: \[\hat{y}^{(pers)}_i = x_i\]
\item Any other simple estimate of the target that could be calculated with reasonable effort given the same inputs as our model.
\item In fact, linear regression itself is often a good reference to benchmark more complicated ML model.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Skill scores}
\begin{itemize}
\item Given a suitable error metric, we calculate
\begin{itemize}
\item \(S\): the error of our model (averaged over a test data set)
\item \(S_{ref}\): the mean error of our chosen reference model 
\item \(S_{perf}\): the mean error or a hypothetical perfect model that outputs \(\hat{y}_i = y_i\) each time; (usually \(S_{perf} = 0\))
\end{itemize}
\item The skill score of our model relative to the benchmark is then defined as
\[Skill = \frac{S_{ref} - S}{S_{ref} - S_{perf}}\]
\item \(Skill \le 0\): Our model is not better than the benchmark.
\item \(Skill \in (0, 1)\): Our model improves over the benchmark. 
\item \(Skill = 1\): Our model is perfect.
\end{itemize}
\end{frame}


\begin{frame}{Skill scores}
MORE HERE Skill scores of linear regression model
\end{frame}



\begin{frame}{Example: k-nearest-neighbor classification}
\begin{itemize}
\item given an input \(\fat{x}^*\) find the \(k\) closest features \(\fat{x}^{(1)}, \dots, \fat{x}^{(k)}\) in the training data, and the corresponding outcomes \(y^{(1)}, \dots, y^{(k)}\)
\item "closeness" between the feature vectors \(\fat{x}^{(i)}\) and \(\fat{x}^{(j)}\) is defined in terms of the Euclidean distance \(\sqrt{\sum_m (x^{(i)}_m - x^{(j)}_m)^2}\)
\item predict \(\hat{y}^*\) as the majority vote over \(y^{(1)}, \dots, y^{(k)}\)
\item ties are resolved by picking the outcome class lowest value or alphabetical rank
\end{itemize}
\end{frame}


\begin{frame}{Example: k-nearest-neighbor classification}
\begin{itemize}
\item For illustration we use data from the "Palmer Penguin" data.
\item Features are penguins' bill length (\(x_1\)) and bill depth (\(x_2\)) and outcome (\(y\)) is the penguin's sex (male or female).
\item The kNN classifier (\(k=5\)) separates the \(x_1\)/\(x_2\) plane into regions for "female" and "male", separated by a decision boundary.
\end{itemize}

\begin{center}
\includegraphics[width=.4\textwidth]{fig/knn-data.png}
\includegraphics[width=.4\textwidth]{fig/knn-k5surface.png}
\end{center}
\end{frame}

\begin{frame}{Example: k-nearest-neighbor classification}
\begin{itemize}
\item The parameter \(k\) controls the "roughness" of the classifier. 
\item Larger values of \(k\) produce smoother decision boundaries.
\begin{center}
\includegraphics[width=.3\textwidth]{fig/knn-data.png}\\
\includegraphics[width=.9\textwidth]{fig/knn-kdependence.png}
\end{center}
\item Is \(k\) a trainable parameter that can be selected by empirical loss minimisation?
\end{itemize}
\end{frame}

\begin{frame}{Example: k-nearest-neighbor classification}
\begin{itemize}
\item Split the data randomly into training and test data set:
\begin{itemize}
\item Only the training data set is used by the kNN classifier.
\item Training and test data are used separately to calculate the 0-1 loss (aka accuracy, aka proportion correct) of the classifier.
\end{itemize}
\end{itemize}
\begin{minipage}{.5\textwidth}%
\begin{center}
\includegraphics[width=\textwidth]{fig/knn-cvloss.png}
\end{center}
\end{minipage}%
\begin{minipage}{.5\textwidth}%
\begin{itemize}
\item The training accuracy is perfect for \(k=1\) (Why?) and goes down as \(k\) increases.
\item The test accuracy is best for \(k\approx 6\).
\end{itemize}
\end{minipage}%
\begin{itemize}
\item The neighborhood parameter \(k\) can be optimised by loss minimisation, but should use a different data set than the one used to define the kNN classifier.
\end{itemize}
\end{frame}


\begin{frame}{In-sample vs Out-of-sample error}
\begin{itemize}
\item Our goal is to train a model that generalises well to new data. 
\item Loss on training data (in-sample error) is too optimistic because the model has seen the data.
\item Overfitting: A model achieves very low training loss by "regurtitating" the training data, including any noise and accidental patterns. Overfitted models generalise poorly to new data.
\item Loss on previously unseen data (out-of sample error) is what we are really interested in.
\item It is good practice to remove a fraction of data (e.g. 20\%) during training and use only for model testing.
\end{itemize}
\end{frame}




\begin{frame}{Further reading}
\begin{itemize}
\item Linear algebra for least squares regression.
\item Alternative loss functions
\item Multiple linear regression for multiple inputs.
\item Logistic regression for classification.
\item Radius-neighborhood classifier and other neighborhood methods.
\item Neighborhood methods for missing data imputation.
\item Uncertainty estimation: Resampling methods, bootstrapping.
\end{itemize}
\end{frame}


\section{Tree-based methods}

\subsection{Decision trees}

\begin{frame}{Background}
\begin{itemize}
\item Tree-based methods are simple and powerful function approximation methods.
\item During training, the feature space is split up into rectangular regions.
\item The model's prediction within each rectangular region is a constant.
\end{itemize}
\end{frame}


\begin{frame}{Decision tree example}
\begin{minipage}{.5\textwidth}
\includegraphics[width=\textwidth]{fig/tree-simpleexample.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{center}
\includegraphics[width=.8\textwidth]{fig/tree-graph.png}
\end{center}
\end{minipage}%
\begin{itemize}
\item Left: Suppose (normalised) precipitation \(x_1\) and temperature \(x_2\) determine survival/death (\(y=1\) or \(0\)) of a plant species by a rectangular "habitable zone" in the \(x_1-x_2\) plane (left).
\item Right: The survival/death classification can be represented by a decision tree (right).
\end{itemize}
\end{frame}


\begin{frame}{Decision tree example}
\begin{minipage}{.5\textwidth}
\begin{center}
\includegraphics[width=.5\textwidth]{fig/tree-simpleexample.png}
\end{center}
\end{minipage}%
\begin{minipage}{.4\textwidth}
\begin{center}
\includegraphics[width=.6\textwidth]{fig/tree-graph.png}
\end{center}
\end{minipage}%
\begin{itemize}
\item Classification function for feature vector \(\fat{x}=(x_1,x_2)\) \[f(\fat{x};{\color{green}\fat\theta}) = \begin{cases} 0 & \text{if}\quad x_1 < {\color{green}0}\\ 0 & \text{if}\quad x_1 > {\color{green}0},\ x_2 > {\color{green}0.5} \\ 0 &\text{if}\quad x_1 > {\color{green}0},\ x_2 < {\color{green}-0.5} \\ 1 & \text{otherwise}\end{cases}\]
\item If decision thresholds are trainable parameters, a decision tree becomes an ML model.
\end{itemize}
\end{frame}


\begin{frame}{Decision tree terminology}
\begin{itemize}
\item \textbf{Split}: Partitioning data based on a feature and threshold 
\item \textbf{Node}: a point in the tree where a decision is made
\item \textbf{Root node}: first split in the tree
\item \textbf{Internal node}: A node that performs a split
\item \textbf{Leaf node}: Final node that outputs a prediction
\item \textbf{Depth}: Number of decision levels from Root node to leaves
\item \textbf{Impurity}: Measure how mixed the targets are within a split
\item \textbf{Information gain}: Reduction in impurity after a split
\end{itemize}
\end{frame}




\begin{frame}[fragile]{Training a decision tree for binary classification} 
\begin{minipage}{.6\textwidth}
\begin{lstlisting}[language=python]
from sklearn.tree import DecisionTreeClassifier

# synthetic data 
# X.shape = (300, 2) -- x1,x2 as columns
# y.shape = (300,) -- targets

# initialise decision tree with max. depth 3
clf = DecisionTreeClassifier(max_depth=3)

# train decision tree
clf.fit(X, y)

# predict
clf.predict(np.array([0.5,0]).reshape(-1,2)) 
# returns: array([1])
\end{lstlisting}
\end{minipage}%
\begin{minipage}{.4\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{fig/tree-simple-fitted.png}
\end{center}
\end{minipage}%

The trained decision tree recovers the correct classification boundary.
\end{frame}


\begin{frame}{WIP: Code example: Tree cover classification}
TODO: Data set (plot), no code required it's on previous slide, decision surface
\end{frame}


\begin{frame}[fragile]{Visualising trained decision trees}
\begin{minipage}{.4\textwidth}
\begin{lstlisting}[language=python]
from sklearn import tree
clf = DecisionTreeClassifier(
        max_depth=3)
clf.fit(X, y)
plt.figure(figsize=(10, 6))
tree.plot_tree(clf,
  feature_names=["altitude", 
                 "aspect"],
  class_names=["not_fir", 
               "fir"],
  filled=True, 
  rounded=True,
  fontsize=8)
plt.show()
\end{lstlisting}
\end{minipage}%
\begin{minipage}{.6\textwidth}
\includegraphics[width=\textwidth]{fig/tree-vis.png}
\end{minipage}%
\end{frame}


\begin{frame}{Finding features and thresholds for splitting}
\begin{minipage}{.6\textwidth}
\begin{itemize}
\item Gini impurity and entropy are measures of impurity, disorder, indeterminism.
\item Binary data: \(\fat{y} = (y_1, \dots, y_n) = (0,0,1,0,1,...)\).
\item Summarise \(\bar{y} = \frac1n \sum_{i=1}^n y_i\).
\end{itemize}
\end{minipage}%
\begin{minipage}{.4\textwidth}
\includegraphics[width=\textwidth]{fig/impurity-sketch.png}
\end{minipage}%

\vfill

\begin{itemize}
\item Gini impurity: \(G(\fat{y}) = 1 - \bar{y}^2 - (1-\bar{y})^2\)
\item Entropy: \(H(\fat{y}) = \bar{y}\log \bar{y} + (1-\bar{y})\log (1-\bar{y})\)
\end{itemize}
\end{frame}


\begin{frame}{Maximising Information Gain}
\begin{itemize}
\item We apply threshold-splitting of the data wrt one feature.
\item This separates the data \(\fat{y}\) into \(\fat{y}_{left}\) and \(\fat{y}_{right}\) of sizes \(n_{left}\) and \(n_{right}\).
\item The impurity of the split data set is calculated by \[G(\fat{y}_{left}, \fat{y}_{right}) = \frac{n_{left}}{n} G(\fat{y}_{left}) + \frac{n_{right}}{n} G(\fat{y}_{right})\]
\item The impurity reduction (or "information gain" \(IG\)) caused by the split is given by \[IG = G(\fat{y}) - G(\fat{y}_{left}, \fat{y}_{right})\]
\item The feature/threshold combination that yields the highest information gain is selected.
\end{itemize}
\end{frame}


\begin{frame}{Maximising Information Gain}
\begin{minipage}{.5\textwidth}
\includegraphics[width=\textwidth]{fig/tree-scatter.png}
\includegraphics[width=\textwidth]{fig/tree-informationgain.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\includegraphics[width=\textwidth]{fig/tree-histalldata.png}
\includegraphics[width=\textwidth]{fig/tree-histsplit.png}
\end{minipage}%
\end{frame}



\subsection{Random Forest, XGBoost}


\begin{frame}{Random Forest}

\begin{itemize}
\item A RF is an ensemble of decision trees.
\item Each tree is trained on random subsets of training data and input features.
\item Each tree makes a prediction and the forest aggregates them.
\begin{itemize}
\item Classification: Majority vote
\item Regression: Average
\end{itemize}
\item Improves robustness and avoids overfitting compared to single decision tree.
\end{itemize}
\end{frame}



\begin{frame}[fragile]{Example}
\begin{lstlisting}[language=python]
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(
    n_estimators=100, # number of trees
    max_depth=20,     # maximum tree depth
    random_state=42   # reproducibility
)
model.fit(X, y)
y_pred = model.predict(X)
\end{lstlisting}
MORE HERE: example data, single tree vs RF decision surfaces
\end{frame}


\begin{frame}{Boosting}
\begin{itemize}
\item In ensemble methods (such as RF), each classifier (tree) is independently trained and optimised for maximum accuracy.
\item A boosting algorithm trains a chain of weak classifiers sequentially. 
\item Core algorithm:
\begin{itemize}
\item train a simple model
\item calculate its errors over the training data
\item train the next model to take the same inputs but predict the previous model's errors
\item repeat, then aggregate all models (often with weights)
\end{itemize}
\item Throughtout the sequence the model is improved by gradually refining the prediction.
\end{itemize}
\end{frame}


\begin{frame}{WIP: Gradient Boosting, XGBoost}
\begin{itemize}
\item Gradients of the loss function \(g_i = \frac{\partial L(\hat{y}_i,y_i)}{\partial \hat{y}_i}\) approximate errors.
\item Changing \(\hat{y}_i\) proportional to \(-g_i\) will decrease the loss.
\item In gradient boosting, each tree predicts the negative gradient \(\frac{\partial L(\hat{y},y)}{\partial \hat{y}}\) of the previous tree
\item XGBoost is a popular library for Gradient Boosting
\item MORE HERE
\end{itemize}
\end{frame}



\begin{frame}{WIP: Forest fire prediction}

\end{frame}

\begin{frame}{WIP: Evaluation metrics in scikit-learn}
\begin{itemize}
\item accuracy
\item confusion matrix
\item ROC/AUC analysis
\item classification report
\item many more for further reading
\end{itemize}
\end{frame}


\begin{frame}{WIP: Variable importance}
\begin{itemize}
\item quick and dirty: clf.feature\_importances\_ (mean impurity decrease)
\item more informative: sklearn.inspection.permutation\_importance (performance drop when a feature is shuffled)
\item "explainability"/reporting: SHAP values (global importance measure, standard)
\item many more for further reading
\end{itemize}
\end{frame}



\section{Neural Networks}

\begin{frame}{Artificial neuron (Perceptron)}
\begin{itemize}
\item An artificial neuron (perceptron) is a function 
\[y = f(\fat{x}; \fat{w}, b) = \varphi\left( w_1 \cdot x_1 + \dots + w_k \cdot x_k + b\right)\]
\item where
\begin{itemize}
\item \(\fat{x} = (x_1, \dots, x_k)\) is the \(k\)-dimensional input vector
\item \(y\) is the 1-dimensional output of the neuron
\item \(\fat{w} = (w_1, \dots, w_k)\) is the \(k\)-dimensional vector of weights 
\item \(b\) is the bias
\item \(\varphi(\cdot)\) is the activation function
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Activation functions}
\begin{itemize}
\item Typical activation functions:
\begin{itemize}
\item ReLU (Rectifier): \(\varphi(x) = max(0,x)\)
\item Sigmoid: \(\varphi(x) = 1 / (1 + e^{-x})\)
\item Swish: \(\varphi(x) = x / (1 + e^{-x})\)
\item Tanh: \(\varphi(x) = (e^x - e^{-x}) / (e^x + e^{-x})\)
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.5\textwidth]{fig/nn-activations.png}
\end{center}
\end{frame}


\begin{frame}{Single-layer perceptron}
\begin{itemize}
\item multiple perceptrons (each with different weights and biases) calculating their outputs in parallel
\[y_i = \varphi\left(\fat{w}_i^T \fat{x} + b_i\right)\] 
\end{itemize}
\begin{center}
\includegraphics[width=.5\textwidth]{fig/nn-single-layer-perceptron.png}
\end{center}
\end{frame}

\begin{frame}{Examples}
Examples of randomly initialised 1d and 2d single layer perceptrons with increasing numbers of neurons, illustrating increasing flexibility and "expressive power".
\begin{center}
\includegraphics[width=.7\textwidth]{fig/nn-randomrelu1d.png}
\includegraphics[width=.7\textwidth]{fig/nn-randomrelu2d.png}
\end{center}
\end{frame}

\begin{frame}{WIP - Terminology}
\begin{itemize}
\item Neuron, node
\item Weight
\item Bias
\item Hidden layer
\item Output layer
\end{itemize}
\end{frame}

\begin{frame}{Multilayer perceptron}
\end{frame}


\begin{frame}{The Keras API}
\end{frame}


\begin{frame}{Simple data example}
\end{frame}

\begin{frame}{Model summary}
\end{frame}


\begin{frame}{Loss functions}
crossentropy-loss for classification, squared-error loss for regression
\end{frame}

\begin{frame}{Gradient descent}
\end{frame}

\begin{frame}{Backpropagation}
\end{frame}


\begin{frame}{Stochastic gradient descent}
\end{frame}


\begin{frame}{Optimiser hyperparameters}
\end{frame}

\begin{frame}{Monitoring performance}
learning curves, restarting training runs 
\end{frame}


\begin{frame}{Spatial data: 2d convolution}
\end{frame}

\begin{frame}{The channel dimension}
\end{frame}

\begin{frame}{Convolutional neural network}
\end{frame}

\begin{frame}{Encoder-Decoder architecture}
\end{frame}

\begin{frame}{Example: Weather image classification}
\end{frame}

\begin{frame}{Cross-validation, early stopping}
\end{frame}

\begin{frame}{Dropout}
\end{frame}


\begin{frame}{Further Reading}
\begin{itemize}
\item Sensitivity analysis, interpretability 
\item Fine-tuning for new outputs/objectives
\item Speeding up NNs: Quantisation, layer-wise optimisation 
\item MORE HERE
\end{itemize}
\end{frame}


\end{document}

